{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of a public dataset of Blood Bowl matches played on FUMBBL.com\n",
    "\n",
    "This blogpost is about **Blood Bowl**, a boardgame I started playing last year. The goal of this blog post is to use Python API and HTML scraping to fetch online Blood Bowl match outcome data, and to create a structured dataset ready for analysis and visualization. This blogpost is written as a Jupyter notebook containing Python code, and is fully reproducible. The idea is to make Blood Bowl data analysis accessible to others. Using open source tooling reduces the barriers for others to build on other people’s work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from isoweek import Week\n",
    "\n",
    "import requests # API library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Bowl online: FUMBBL \n",
    "\n",
    "The **FUMBBL** website (https://fumbbl.com) is one big pile of data. From coach pages, with their teams, to team rosters, with players, and match histories. It's all there.\n",
    "\n",
    "To obtain **FUMBBL** data, we need to fetch it match by match, team by team. To do so, the site creator Christer Kaivo-oja, from Sweden, has made an API that allows us to easily fetch data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python requests package to fetch data\n",
    "We use the [Python **Requests** library](https://docs.python-requests.org/en/latest/) to make the API call over HTTPS and obtain the response from the FUMBLL server. The response is in the JSON format, a [light-weight data-interchange format](https://www.json.org/json-en.html) which is both easy to read and write for humans, and easy to parse and generate by computers. So this makes it a natural choice for an API.\n",
    "The full documentation of the API can be found at (https://fumbbl.com/apidoc/).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "We go with a flat data frame with **rows for each match**, and columns for the various variables associated with each match.\n",
    "These would include:\n",
    "\n",
    "* Coach ids\n",
    "* Team races\n",
    "* Team ids\n",
    "* Date of the match\n",
    "* Outcome (Touchdowns of both teams)\n",
    "\n",
    "With this basic structure, we can add as many match related variables in the future, keeping the basic structure (each row is a match) unchanged.\n",
    "\n",
    "So lets get the match data!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: API scraping the match data: df_matches\n",
    "\n",
    "So we are mostly interested in the current ruleset, this is `BB2020`. This ruleset became available in **FUMBBL** at september 1st 2021, and two months later, some 5000 games have been played. We also want to compare with the previous ruleset, where we have much more data available. \n",
    "The dataset start with match `4216258` played on august 1st, 2020. This covers roughly 12 months of `BB2016` ruleset matches, after that it switches to predominantly `BB2020` matches.\n",
    "\n",
    "We collect match data by looping over `match_id`. We store the full JSON file on disk, so we avoid repeat API calls in the future.\n",
    "\n",
    "**VERY IMPORTANT: We do not want to overload the **FUMBBL** server, so we make only three API requests per second. In this way, the server load is hardly affected and it can continue functioning properly for all the Blood Bowl coaches playing their daily games!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run write_json_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/df_matches_20230524_191844.h5\n",
      "matches to grab:\n",
      "31625\n"
     ]
    }
   ],
   "source": [
    "# PM split out fetching JSON and transforming to pandas\n",
    "df_matches = pd.DataFrame(columns=['match_id', 'replay_id', 'tournament_id', 'match_date', 'match_time',  'match_conceded',\n",
    "    'team1_id', 'team1_coach_id', 'team1_roster_id', 'team1_race_name', 'team1_value', 'team1_cas_bh', 'team1_cas_si', 'team1_cas_rip',\n",
    "    'team2_id', 'team2_coach_id', 'team2_roster_id', 'team2_race_name', 'team2_value', 'team2_cas_bh', 'team2_cas_si', 'team2_cas_rip',\n",
    "    'team1_score', 'team2_score'])\n",
    "\n",
    "target = 'raw/df_matches_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "print(target)\n",
    "\n",
    "end_match = 4460944 \n",
    "begin_match = 4429319\n",
    "n_matches = end_match - begin_match\n",
    "full_run = 0\n",
    "print(\"matches to grab:\")\n",
    "print(n_matches)\n",
    "\n",
    "if(full_run):\n",
    "    for i in range(n_matches):\n",
    "        match_id = end_match - i\n",
    "        api_string = \"https://fumbbl.com/api/match/get/\" + str(match_id)\n",
    "\n",
    "        match = requests.get(api_string)\n",
    "        match = match.json()\n",
    "\n",
    "        dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        fname_string = dirname + \"/match_\" + str(match_id) + \".json\"\n",
    "\n",
    "        write_json_file(match, fname_string)\n",
    "\n",
    "        if match: # fix for matches that do not exist\n",
    "            match_id = match['id']\n",
    "            replay_id = match['replayId']\n",
    "            tournament_id = match['tournamentId'] # key to tournament table\n",
    "            match_date = match['date']\n",
    "            match_time = match['time']\n",
    "            match_conceded = match['conceded']\n",
    "            team1_id = match['team1']['id']\n",
    "            team2_id = match['team2']['id']\n",
    "            # touchdowns\n",
    "            team1_score = match['team1']['score']\n",
    "            team2_score = match['team2']['score']  \n",
    "            # casualties\n",
    "            team1_cas_bh = match['team1']['casualties']['bh']\n",
    "            team1_cas_si = match['team1']['casualties']['si']\n",
    "            team1_cas_rip = match['team1']['casualties']['rip']\n",
    "            team2_cas_bh = match['team2']['casualties']['bh']\n",
    "            team2_cas_si = match['team2']['casualties']['si']\n",
    "            team2_cas_rip = match['team2']['casualties']['rip']\n",
    "            # other\n",
    "            team1_roster_id = match['team1']['roster']['id']\n",
    "            team2_roster_id = match['team2']['roster']['id']            \n",
    "            team1_coach_id = match['team1']['coach']['id']\n",
    "            team2_coach_id = match['team2']['coach']['id']\n",
    "            team1_race_name = match['team1']['roster']['name'] \n",
    "            team2_race_name = match['team2']['roster']['name'] \n",
    "            team1_value = match['team1']['teamValue']\n",
    "            team2_value = match['team2']['teamValue']\n",
    "            #print(match_id)     \n",
    "            df_matches.loc[i] = [match_id, replay_id, tournament_id, match_date, match_time, match_conceded,\n",
    "                team1_id, team1_coach_id, team1_roster_id, team1_race_name, team1_value, team1_cas_bh, team1_cas_si, team1_cas_rip, \n",
    "                team2_id, team2_coach_id, team2_roster_id, team2_race_name, team2_value, team2_cas_bh, team2_cas_si, team2_cas_rip, \n",
    "                team1_score, team2_score]\n",
    "        else:\n",
    "            # empty data for this match, create empty row\n",
    "            match_id = int(end_match - i)\n",
    "            df_matches.loc[i] = np.repeat([np.NaN], 24, axis=0)\n",
    "            df_matches.loc[i]['match_id'] = int(match_id)\n",
    "        if i % 100 == 0: \n",
    "            # write tmp data as hdf5 file\n",
    "            print(i, end='')\n",
    "            print(\".\", end='')\n",
    "            df_matches.to_hdf(target, key='df_matches', mode='w')\n",
    "\n",
    "    # write data as hdf5 file\n",
    "    df_matches.to_hdf(target, key='df_matches', mode='w')\n",
    "else:\n",
    "    # read from hdf5 file\n",
    "    df_matches1 = pd.read_hdf('raw/df_matches_20220310_155600.h5') # 4216259 - 4221258\n",
    "    df_matches2 = pd.read_hdf('raw/df_matches_20220316_180506.h5') # 4221259 - 4221820\n",
    "    df_matches3 = pd.read_hdf('raw/df_matches_20220311_084424.h5') # 4221821 - 4271820\n",
    "    df_matches4 = pd.read_hdf('raw/df_matches_20220311_231408.h5') # 4271821 - 4321820\n",
    "    df_matches5 = pd.read_hdf('raw/df_matches_20220312_083221.h5') # 4321821 - 4370543\n",
    "    df_matches6 = pd.read_hdf('raw/df_matches_20220606_225206.h5') # 4374338 - 4386470\n",
    "    df_matches7 = pd.read_hdf('raw/df_matches_20220607_060907.h5') # 4370544 - 4374337\n",
    "    df_matches8 = pd.read_hdf('raw/df_matches_20230108_205542.h5') # 4429319 - 4386470\n",
    "    df_matches9 = pd.read_hdf('raw/df_matches_20230523_222012.h5') # 4460944 - 4429319\n",
    "    df_matches = pd.concat([df_matches1, df_matches2, df_matches3, \n",
    "        df_matches4, df_matches5, df_matches6, df_matches7, df_matches8, df_matches9], ignore_index=True)\n",
    "\n",
    "df_matches.shape\n",
    "df_matches = df_matches.sort_values(by=['match_id']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244652, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>replay_id</th>\n",
       "      <th>tournament_id</th>\n",
       "      <th>match_date</th>\n",
       "      <th>match_time</th>\n",
       "      <th>match_conceded</th>\n",
       "      <th>team1_id</th>\n",
       "      <th>team1_coach_id</th>\n",
       "      <th>team1_roster_id</th>\n",
       "      <th>team1_race_name</th>\n",
       "      <th>team1_value</th>\n",
       "      <th>team1_cas_bh</th>\n",
       "      <th>team1_cas_si</th>\n",
       "      <th>team1_cas_rip</th>\n",
       "      <th>team2_id</th>\n",
       "      <th>team2_coach_id</th>\n",
       "      <th>team2_roster_id</th>\n",
       "      <th>team2_race_name</th>\n",
       "      <th>team2_value</th>\n",
       "      <th>team2_cas_bh</th>\n",
       "      <th>team2_cas_si</th>\n",
       "      <th>team2_cas_rip</th>\n",
       "      <th>team1_score</th>\n",
       "      <th>team2_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244651</th>\n",
       "      <td>4460944</td>\n",
       "      <td>1622510</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>04:52:59</td>\n",
       "      <td>None</td>\n",
       "      <td>1116389</td>\n",
       "      <td>107478</td>\n",
       "      <td>4965</td>\n",
       "      <td>Imperial Nobility</td>\n",
       "      <td>1450k</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1122664</td>\n",
       "      <td>258292</td>\n",
       "      <td>4956</td>\n",
       "      <td>Black Orc</td>\n",
       "      <td>1740k</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       match_id replay_id tournament_id  match_date match_time match_conceded  \\\n",
       "244651  4460944   1622510             0  2023-05-23   04:52:59           None   \n",
       "\n",
       "       team1_id team1_coach_id team1_roster_id    team1_race_name team1_value  \\\n",
       "244651  1116389         107478            4965  Imperial Nobility       1450k   \n",
       "\n",
       "       team1_cas_bh team1_cas_si team1_cas_rip team2_id team2_coach_id  \\\n",
       "244651            0            0             0  1122664         258292   \n",
       "\n",
       "       team2_roster_id team2_race_name team2_value team2_cas_bh team2_cas_si  \\\n",
       "244651            4956       Black Orc       1740k            0            2   \n",
       "\n",
       "       team2_cas_rip team1_score team2_score  \n",
       "244651             0           0           2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matches.query('match_id == 4460944')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep: fixing the datatypes, creating derived variables\n",
    "\n",
    "Since we manually filled the `pandas` DataFrame, most of the columns are now of `object` datatype.\n",
    "We need to change this to be able to work properly with the data, as well as store it properly.\n",
    "Here I convert each column manually, however I later found out about `DataFrame.infer_objects()`, that can detect the proper dtype automatically.\n",
    "This I will try next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert object dtype columns to proper pandas dtypes datetime and numeric\n",
    "df_matches['match_date'] = pd.to_datetime(df_matches.match_date) # Datetime object\n",
    "df_matches['match_id'] = pd.to_numeric(df_matches.match_id) \n",
    "df_matches['replay_id'] = pd.to_numeric(df_matches.replay_id) \n",
    "df_matches['tournament_id'] = pd.to_numeric(df_matches.tournament_id) \n",
    "df_matches['team1_id'] = pd.to_numeric(df_matches.team1_id) \n",
    "df_matches['team1_coach_id'] = pd.to_numeric(df_matches.team1_coach_id) \n",
    "df_matches['team1_roster_id'] = pd.to_numeric(df_matches.team1_roster_id) \n",
    "df_matches['team2_id'] = pd.to_numeric(df_matches.team2_id) \n",
    "df_matches['team2_coach_id'] = pd.to_numeric(df_matches.team2_coach_id) \n",
    "df_matches['team2_roster_id'] = pd.to_numeric(df_matches.team2_roster_id) \n",
    "df_matches['team1_score'] = pd.to_numeric(df_matches.team1_score) \n",
    "df_matches['team2_score'] = pd.to_numeric(df_matches.team2_score) \n",
    "\n",
    "# calculate match score difference\n",
    "df_matches['team1_win'] = np.sign(df_matches['team1_score'] - df_matches['team2_score'])\n",
    "df_matches['team2_win'] = np.sign(df_matches['team2_score'] - df_matches['team1_score'])\n",
    "\n",
    "# mirror match\n",
    "df_matches['mirror_match'] = 0\n",
    "df_matches.loc[df_matches['team1_race_name'] == df_matches['team2_race_name'], 'mirror_match'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244652 entries, 0 to 244651\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   match_id         244652 non-null  int64         \n",
      " 1   replay_id        244612 non-null  float64       \n",
      " 2   tournament_id    244612 non-null  float64       \n",
      " 3   match_date       244612 non-null  datetime64[ns]\n",
      " 4   match_time       244612 non-null  object        \n",
      " 5   match_conceded   244612 non-null  object        \n",
      " 6   team1_id         244612 non-null  float64       \n",
      " 7   team1_coach_id   244612 non-null  float64       \n",
      " 8   team1_roster_id  244612 non-null  float64       \n",
      " 9   team1_race_name  244612 non-null  object        \n",
      " 10  team1_value      244612 non-null  object        \n",
      " 11  team1_cas_bh     244612 non-null  object        \n",
      " 12  team1_cas_si     244612 non-null  object        \n",
      " 13  team1_cas_rip    244612 non-null  object        \n",
      " 14  team2_id         244612 non-null  float64       \n",
      " 15  team2_coach_id   244612 non-null  float64       \n",
      " 16  team2_roster_id  244612 non-null  float64       \n",
      " 17  team2_race_name  244612 non-null  object        \n",
      " 18  team2_value      244612 non-null  object        \n",
      " 19  team2_cas_bh     244612 non-null  object        \n",
      " 20  team2_cas_si     244612 non-null  object        \n",
      " 21  team2_cas_rip    244612 non-null  object        \n",
      " 22  team1_score      244612 non-null  float64       \n",
      " 23  team2_score      244612 non-null  float64       \n",
      " 24  team1_win        244612 non-null  float64       \n",
      " 25  team2_win        244612 non-null  float64       \n",
      " 26  mirror_match     244652 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(12), int64(2), object(12)\n",
      "memory usage: 50.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches['team1_cas_bh'] = pd.to_numeric(df_matches.team1_cas_bh) \n",
    "df_matches['team1_cas_si'] = pd.to_numeric(df_matches.team1_cas_si) \n",
    "df_matches['team1_cas_rip'] = pd.to_numeric(df_matches.team1_cas_rip) \n",
    "# add total CAS\n",
    "df_matches['team1_cas'] = df_matches['team1_cas_bh'] + df_matches['team1_cas_si'] + df_matches['team1_cas_rip']\n",
    "\n",
    "\n",
    "df_matches['team2_cas_bh'] = pd.to_numeric(df_matches.team2_cas_bh) \n",
    "df_matches['team2_cas_si'] = pd.to_numeric(df_matches.team2_cas_si) \n",
    "df_matches['team2_cas_rip'] = pd.to_numeric(df_matches.team2_cas_rip) \n",
    "# add total CAS\n",
    "df_matches['team2_cas'] = df_matches['team2_cas_bh'] + df_matches['team2_cas_si'] + df_matches['team2_cas_rip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8870, 29)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mirror matches\n",
    "df_matches.query('mirror_match == 1').shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: transforming the team values\n",
    "\n",
    "In Blood Bowl, teams can develop themselves over the course of multiple matches. The winnings of each match can be spend on buying new, stronger players, or replace the players that ended up getting injured or even killed. In addition, players receive so-called *star points* for important events, such as scoring, or inflicting a casualty on the opponent. Therefore, a balancing mechanism is needed when a newly created \"rookie\" team is facing a highly developed opposing team with lots of extra skills and strong players. \n",
    "\n",
    "Blood Bowl solves this by calculating for both teams their **Current team value**.\n",
    "The **Team value difference** for a match determines the amount of gold that the weaker team can use to buy so-called **inducements**.\n",
    "These inducements are temporary, and can consists of a famous \"star player\" who joins the team just for this match. Another popular option is to hire a wizard that can be used to turn one of the opposing players into a frog.\n",
    "\n",
    "It is well known that the win rates of the teams depend on how developed a team is. For example, Amazons are thought to be strongest at low team value, as they already start out with lots of *block* and *dodge* skills, whereas a Chaos team start out with almost no skills.\n",
    "So if we compare win rates, we would like take into account the current team value. \n",
    "Now as this can differ between the two teams in a match up, I reasoned that the highest team value is most informative about the average strength level of both teams, because of the inducement mechanism described above. (In the next step, we will add information on inducements)\n",
    "\n",
    "In the dataset, we have for each match the current team values of both teams as a text string. \n",
    "We transform the text string `1100k` into an integer number `1100`, so that we can calculated the difference as `tv_diff`, and pick for each match the maximum team value and store it as `tv_match`. Finally, we create a team value bin `tv_bin` to be able to compare win rates for binned groups of matches where races have comparable team strength / team development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert team value 1100k to 1100 integer and and above / below median (= low / high TV)\n",
    "df_matches['team1_value'] = df_matches['team1_value'].str.replace('k$', '')\n",
    "df_matches['team1_value'] = df_matches['team1_value'].fillna(0).astype(np.int64)\n",
    "\n",
    "df_matches['team2_value'] = df_matches['team2_value'].str.replace('k$', '')\n",
    "df_matches['team2_value'] = df_matches['team2_value'].fillna(0).astype(np.int64)\n",
    "\n",
    "df_matches['tv_diff'] = np.abs(df_matches['team2_value'] - df_matches['team1_value'])\n",
    "df_matches['tv_diff2'] = df_matches['team2_value'] - df_matches['team1_value']\n",
    "\n",
    "df_matches['tv_match'] = df_matches[[\"team1_value\", \"team2_value\"]].max(axis=1)\n",
    "\n",
    "df_matches['tv_bin'] = pd.cut(df_matches['tv_match'], \n",
    "    bins = [0, 950, 1250,1550, 1850, float(\"inf\")], \n",
    "    labels=['< 950K', '1.1M', '1.4M', '1.7M', '> 1850K']\n",
    ")\n",
    "\n",
    "df_matches['tv_bin2'] = pd.cut(df_matches['tv_match'], \n",
    "    bins = [0, 950, 1050, 1150, 1250, 1350, 1450, 1550, float(\"inf\")], \n",
    "    labels=['< 950K', '1.0M', '1.1M', '1.2M',  '1.3M', '1.4M', '1.5M', '> 1550K']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping empty matches\n",
    "\n",
    "Some match_id's do not have match information attached to them, presumably these matches were not played or some real life event interfered. These match_ids are dropped from the dataset to get rid of the NAs in all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_matches)\n",
    "df_matches = df_matches.dropna(subset=['match_date'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: getting the dates right\n",
    "\n",
    "To see time trends, its useful to aggregate the data by week. For this we add `week_number` for each date, and from this week number, we convert back to a date to get a `week_date`. This last part is useful for plotting with `plotnine`, as this treats dates in a special way\n",
    "We use the ISO definition of week, this has some unexpected behavior near the beginning / end of each year. \n",
    "\n",
    "The data starts in week 36 (september) of 2020, and stops halfway march 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches['week_number'] = df_matches['match_date'].dt.isocalendar().week\n",
    "\n",
    "# cannot serialize numpy int OR Int64 when writing HDF5 file, so we go for plain int as all NAs are gone now\n",
    "df_matches['week_number'] = df_matches['week_number'].fillna(0).astype(int)\n",
    "\n",
    "# add year based on match ISO week\n",
    "df_matches['year'] = df_matches['match_date'].dt.isocalendar().year.astype(int)\n",
    "\n",
    "df_matches['week_year'] = df_matches['year'].astype(str) + '-' + df_matches['week_number'].astype(str)\n",
    "\n",
    "# use a lambda function since isoweek.Week is not vectorized \n",
    "df_matches['week_date'] = pd.to_datetime(df_matches.apply(lambda row : Week(int(row[\"year\"]),int(row[\"week_number\"])).monday(),axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>replay_id</th>\n",
       "      <th>tournament_id</th>\n",
       "      <th>match_date</th>\n",
       "      <th>match_time</th>\n",
       "      <th>match_conceded</th>\n",
       "      <th>team1_id</th>\n",
       "      <th>team1_coach_id</th>\n",
       "      <th>team1_roster_id</th>\n",
       "      <th>team1_race_name</th>\n",
       "      <th>team1_value</th>\n",
       "      <th>team1_cas_bh</th>\n",
       "      <th>team1_cas_si</th>\n",
       "      <th>team1_cas_rip</th>\n",
       "      <th>team2_id</th>\n",
       "      <th>team2_coach_id</th>\n",
       "      <th>team2_roster_id</th>\n",
       "      <th>team2_race_name</th>\n",
       "      <th>team2_value</th>\n",
       "      <th>team2_cas_bh</th>\n",
       "      <th>team2_cas_si</th>\n",
       "      <th>team2_cas_rip</th>\n",
       "      <th>team1_score</th>\n",
       "      <th>team2_score</th>\n",
       "      <th>team1_win</th>\n",
       "      <th>team2_win</th>\n",
       "      <th>mirror_match</th>\n",
       "      <th>team1_cas</th>\n",
       "      <th>team2_cas</th>\n",
       "      <th>tv_diff</th>\n",
       "      <th>tv_diff2</th>\n",
       "      <th>tv_match</th>\n",
       "      <th>tv_bin</th>\n",
       "      <th>tv_bin2</th>\n",
       "      <th>week_number</th>\n",
       "      <th>year</th>\n",
       "      <th>week_year</th>\n",
       "      <th>week_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231124</th>\n",
       "      <td>4447417</td>\n",
       "      <td>1606437.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>20:40:35</td>\n",
       "      <td>None</td>\n",
       "      <td>1116987.0</td>\n",
       "      <td>133227.0</td>\n",
       "      <td>4978.0</td>\n",
       "      <td>Underworld Denizens</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1118830.0</td>\n",
       "      <td>243311.0</td>\n",
       "      <td>5145.0</td>\n",
       "      <td>Tomb Kings</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>13</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-13</td>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231141</th>\n",
       "      <td>4447434</td>\n",
       "      <td>1606445.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>21:40:02</td>\n",
       "      <td>None</td>\n",
       "      <td>1117780.0</td>\n",
       "      <td>220632.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>Shambling Undead</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1118822.0</td>\n",
       "      <td>131879.0</td>\n",
       "      <td>4979.0</td>\n",
       "      <td>Wood Elf</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>13</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-13</td>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231146</th>\n",
       "      <td>4447439</td>\n",
       "      <td>1606455.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>21:48:45</td>\n",
       "      <td>None</td>\n",
       "      <td>1117560.0</td>\n",
       "      <td>28022.0</td>\n",
       "      <td>4969.0</td>\n",
       "      <td>Necromantic Horror</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1117426.0</td>\n",
       "      <td>5542.0</td>\n",
       "      <td>4979.0</td>\n",
       "      <td>Wood Elf</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>13</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-13</td>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231158</th>\n",
       "      <td>4447451</td>\n",
       "      <td>1606462.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>22:14:44</td>\n",
       "      <td>None</td>\n",
       "      <td>1119081.0</td>\n",
       "      <td>27388.0</td>\n",
       "      <td>4963.0</td>\n",
       "      <td>Halfling</td>\n",
       "      <td>760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1118926.0</td>\n",
       "      <td>226515.0</td>\n",
       "      <td>4972.0</td>\n",
       "      <td>Old World Alliance</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>13</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-13</td>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231184</th>\n",
       "      <td>4447477</td>\n",
       "      <td>1606515.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>23:02:02</td>\n",
       "      <td>None</td>\n",
       "      <td>1117356.0</td>\n",
       "      <td>253629.0</td>\n",
       "      <td>5145.0</td>\n",
       "      <td>Tomb Kings</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1118837.0</td>\n",
       "      <td>225446.0</td>\n",
       "      <td>4959.0</td>\n",
       "      <td>Dark Elf</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>13</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-13</td>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241177</th>\n",
       "      <td>4457470</td>\n",
       "      <td>1618388.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>22:22:54</td>\n",
       "      <td>None</td>\n",
       "      <td>1117894.0</td>\n",
       "      <td>9714.0</td>\n",
       "      <td>4959.0</td>\n",
       "      <td>Dark Elf</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1119013.0</td>\n",
       "      <td>247051.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>Shambling Undead</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>18</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-18</td>\n",
       "      <td>2023-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241178</th>\n",
       "      <td>4457471</td>\n",
       "      <td>1618380.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>22:23:16</td>\n",
       "      <td>None</td>\n",
       "      <td>1118910.0</td>\n",
       "      <td>133484.0</td>\n",
       "      <td>4974.0</td>\n",
       "      <td>Orc</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1118382.0</td>\n",
       "      <td>246903.0</td>\n",
       "      <td>4966.0</td>\n",
       "      <td>Lizardmen</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>18</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-18</td>\n",
       "      <td>2023-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241180</th>\n",
       "      <td>4457473</td>\n",
       "      <td>1618342.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>22:26:39</td>\n",
       "      <td>None</td>\n",
       "      <td>1117987.0</td>\n",
       "      <td>21930.0</td>\n",
       "      <td>4978.0</td>\n",
       "      <td>Underworld Denizens</td>\n",
       "      <td>770</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1118392.0</td>\n",
       "      <td>139015.0</td>\n",
       "      <td>5145.0</td>\n",
       "      <td>Tomb Kings</td>\n",
       "      <td>1150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>380</td>\n",
       "      <td>380</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>18</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-18</td>\n",
       "      <td>2023-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241186</th>\n",
       "      <td>4457479</td>\n",
       "      <td>1618379.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>22:47:34</td>\n",
       "      <td>None</td>\n",
       "      <td>1118124.0</td>\n",
       "      <td>136893.0</td>\n",
       "      <td>4958.0</td>\n",
       "      <td>Chaos Renegade</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1119002.0</td>\n",
       "      <td>246077.0</td>\n",
       "      <td>4966.0</td>\n",
       "      <td>Lizardmen</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>18</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-18</td>\n",
       "      <td>2023-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241198</th>\n",
       "      <td>4457491</td>\n",
       "      <td>1618413.0</td>\n",
       "      <td>59383.0</td>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>23:18:02</td>\n",
       "      <td>None</td>\n",
       "      <td>1117152.0</td>\n",
       "      <td>31796.0</td>\n",
       "      <td>4959.0</td>\n",
       "      <td>Dark Elf</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1117580.0</td>\n",
       "      <td>86572.0</td>\n",
       "      <td>4976.0</td>\n",
       "      <td>Skaven</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>1.1M</td>\n",
       "      <td>18</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-18</td>\n",
       "      <td>2023-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        match_id  replay_id  tournament_id match_date match_time  \\\n",
       "231124   4447417  1606437.0        59383.0 2023-03-27   20:40:35   \n",
       "231141   4447434  1606445.0        59383.0 2023-03-27   21:40:02   \n",
       "231146   4447439  1606455.0        59383.0 2023-03-27   21:48:45   \n",
       "231158   4447451  1606462.0        59383.0 2023-03-27   22:14:44   \n",
       "231184   4447477  1606515.0        59383.0 2023-03-27   23:02:02   \n",
       "...          ...        ...            ...        ...        ...   \n",
       "241177   4457470  1618388.0        59383.0 2023-05-07   22:22:54   \n",
       "241178   4457471  1618380.0        59383.0 2023-05-07   22:23:16   \n",
       "241180   4457473  1618342.0        59383.0 2023-05-07   22:26:39   \n",
       "241186   4457479  1618379.0        59383.0 2023-05-07   22:47:34   \n",
       "241198   4457491  1618413.0        59383.0 2023-05-07   23:18:02   \n",
       "\n",
       "       match_conceded   team1_id  team1_coach_id  team1_roster_id  \\\n",
       "231124           None  1116987.0        133227.0           4978.0   \n",
       "231141           None  1117780.0        220632.0           4975.0   \n",
       "231146           None  1117560.0         28022.0           4969.0   \n",
       "231158           None  1119081.0         27388.0           4963.0   \n",
       "231184           None  1117356.0        253629.0           5145.0   \n",
       "...               ...        ...             ...              ...   \n",
       "241177           None  1117894.0          9714.0           4959.0   \n",
       "241178           None  1118910.0        133484.0           4974.0   \n",
       "241180           None  1117987.0         21930.0           4978.0   \n",
       "241186           None  1118124.0        136893.0           4958.0   \n",
       "241198           None  1117152.0         31796.0           4959.0   \n",
       "\n",
       "            team1_race_name  team1_value  team1_cas_bh  team1_cas_si  \\\n",
       "231124  Underworld Denizens         1000           0.0           2.0   \n",
       "231141     Shambling Undead         1150           0.0           1.0   \n",
       "231146   Necromantic Horror         1150           1.0           1.0   \n",
       "231158             Halfling          760           1.0           1.0   \n",
       "231184           Tomb Kings         1150           1.0           0.0   \n",
       "...                     ...          ...           ...           ...   \n",
       "241177             Dark Elf         1150           1.0           0.0   \n",
       "241178                  Orc         1150           0.0           1.0   \n",
       "241180  Underworld Denizens          770           3.0           2.0   \n",
       "241186       Chaos Renegade         1150           0.0           0.0   \n",
       "241198             Dark Elf         1150           0.0           0.0   \n",
       "\n",
       "        team1_cas_rip   team2_id  team2_coach_id  team2_roster_id  \\\n",
       "231124            0.0  1118830.0        243311.0           5145.0   \n",
       "231141            1.0  1118822.0        131879.0           4979.0   \n",
       "231146            1.0  1117426.0          5542.0           4979.0   \n",
       "231158            0.0  1118926.0        226515.0           4972.0   \n",
       "231184            1.0  1118837.0        225446.0           4959.0   \n",
       "...               ...        ...             ...              ...   \n",
       "241177            0.0  1119013.0        247051.0           4975.0   \n",
       "241178            1.0  1118382.0        246903.0           4966.0   \n",
       "241180            0.0  1118392.0        139015.0           5145.0   \n",
       "241186            0.0  1119002.0        246077.0           4966.0   \n",
       "241198            0.0  1117580.0         86572.0           4976.0   \n",
       "\n",
       "           team2_race_name  team2_value  team2_cas_bh  team2_cas_si  \\\n",
       "231124          Tomb Kings         1150           1.0           0.0   \n",
       "231141            Wood Elf         1150           0.0           0.0   \n",
       "231146            Wood Elf         1150           0.0           0.0   \n",
       "231158  Old World Alliance         1150           0.0           2.0   \n",
       "231184            Dark Elf         1150           0.0           1.0   \n",
       "...                    ...          ...           ...           ...   \n",
       "241177    Shambling Undead         1150           0.0           2.0   \n",
       "241178           Lizardmen         1150           0.0           0.0   \n",
       "241180          Tomb Kings         1150           2.0           1.0   \n",
       "241186           Lizardmen         1150           0.0           0.0   \n",
       "241198              Skaven         1150           0.0           1.0   \n",
       "\n",
       "        team2_cas_rip  team1_score  team2_score  team1_win  team2_win  \\\n",
       "231124            1.0          1.0          0.0        1.0       -1.0   \n",
       "231141            1.0          0.0          3.0       -1.0        1.0   \n",
       "231146            0.0          1.0          1.0        0.0        0.0   \n",
       "231158            1.0          1.0          1.0        0.0        0.0   \n",
       "231184            0.0          0.0          1.0       -1.0        1.0   \n",
       "...               ...          ...          ...        ...        ...   \n",
       "241177            0.0          2.0          0.0        1.0       -1.0   \n",
       "241178            0.0          0.0          0.0        0.0        0.0   \n",
       "241180            0.0          2.0          1.0        1.0       -1.0   \n",
       "241186            0.0          0.0          2.0       -1.0        1.0   \n",
       "241198            0.0          1.0          1.0        0.0        0.0   \n",
       "\n",
       "        mirror_match  team1_cas  team2_cas  tv_diff  tv_diff2  tv_match  \\\n",
       "231124             0        2.0        2.0      150       150      1150   \n",
       "231141             0        2.0        1.0        0         0      1150   \n",
       "231146             0        3.0        0.0        0         0      1150   \n",
       "231158             0        2.0        3.0      390       390      1150   \n",
       "231184             0        2.0        1.0        0         0      1150   \n",
       "...              ...        ...        ...      ...       ...       ...   \n",
       "241177             0        1.0        2.0        0         0      1150   \n",
       "241178             0        2.0        0.0        0         0      1150   \n",
       "241180             0        5.0        3.0      380       380      1150   \n",
       "241186             0        0.0        0.0        0         0      1150   \n",
       "241198             0        0.0        1.0        0         0      1150   \n",
       "\n",
       "       tv_bin tv_bin2  week_number  year week_year  week_date  \n",
       "231124   1.1M    1.1M           13  2023   2023-13 2023-03-27  \n",
       "231141   1.1M    1.1M           13  2023   2023-13 2023-03-27  \n",
       "231146   1.1M    1.1M           13  2023   2023-13 2023-03-27  \n",
       "231158   1.1M    1.1M           13  2023   2023-13 2023-03-27  \n",
       "231184   1.1M    1.1M           13  2023   2023-13 2023-03-27  \n",
       "...       ...     ...          ...   ...       ...        ...  \n",
       "241177   1.1M    1.1M           18  2023   2023-18 2023-05-01  \n",
       "241178   1.1M    1.1M           18  2023   2023-18 2023-05-01  \n",
       "241180   1.1M    1.1M           18  2023   2023-18 2023-05-01  \n",
       "241186   1.1M    1.1M           18  2023   2023-18 2023-05-01  \n",
       "241198   1.1M    1.1M           18  2023   2023-18 2023-05-01  \n",
       "\n",
       "[782 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matches.query('tournament_id == 59383')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: HTML Scraping more match related data\n",
    "\n",
    "Next, we collect more match related data to add to  `df_matches`.\n",
    "For example, the **inducements** and **coach rankings**, as well as match performance stats such as total passing distance `pass`. \n",
    "This information is not available through the API, but each played match has an associated HTML page at https://fumbbl.com/FUMBBL.php?page=match with more info.\n",
    "\n",
    "Since we have to fetch the complete HTML page for each match anyways, I decided to split the proces in two steps:\n",
    "\n",
    "In the first step, the HTML pages for the desired matches are fetched and stored on disk. \n",
    "After some optimization fetching 1K matches takes 10 min. So 6K matches per hour.\n",
    "\n",
    "(Total file size for 154K files is 7GB. These files cannot be stored on Github.)\n",
    "\n",
    "In the second step, the HTML pages are processed with `BeautifulSoup` to extract inducments and coach rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://thehftguy.com/2020/07/28/making-beautifulsoup-parsing-10-times-faster/\n",
    "\n",
    "import lxml\n",
    "import cchardet\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import gzip\n",
    "\n",
    "end_match =  4460944\n",
    "begin_match = 4429319\n",
    "\n",
    "print(\"matches to scrape: \")\n",
    "n_matches = end_match - begin_match\n",
    "\n",
    "full_run = 0\n",
    "\n",
    "print(n_matches)\n",
    "\n",
    "if(full_run):\n",
    "    for i in range(n_matches):\n",
    "        match_id = end_match - i\n",
    "        api_string = \"https://fumbbl.com/FUMBBL.php?page=match&id=\" + str(match_id)\n",
    "\n",
    "        response = requests.get(api_string)\n",
    "\n",
    "        dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        fname_string = dirname + \"/match_\" + str(match_id) + \".html.gz\"\n",
    "        \n",
    "        with gzip.open(fname_string, mode = \"wb\") as f:\n",
    "            f.write(response.text.encode(\"utf-8\"))\n",
    "            f.close()\n",
    "\n",
    "        if i % 1000 == 0: \n",
    "            # write progress report\n",
    "            print(i, end='')\n",
    "            print(\".\", end='')\n",
    "\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2b: extract player ids from match data (NOT ACTIVE RIGHT NOW)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we write a function that, given a match_id, reads the contents in BeautifulSoup and extracts the stuff we need.\n",
    "It returns them as separate lists, later to be combined in a pd df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_player_ids_from_single_file(match_id):\n",
    "    \n",
    "    dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "    fname_string = dirname + \"/match_\" + str(match_id) + \".html\"  \n",
    "\n",
    "    with open(fname_string, mode = \"r\") as f:\n",
    "        # https://stackoverflow.com/questions/30277109/beautifulsoup-takes-forever-can-this-be-done-faster\n",
    "        soup = BeautifulSoup(f, 'xml')\n",
    "\n",
    "    if soup.find(\"div\", {\"class\": \"matchrecord\"}) is not None:\n",
    "        # match record is available\n",
    "        player_id = []\n",
    "        player_number = []\n",
    "        player_name = []\n",
    "\n",
    "        players = soup.find_all(\"div\", class_=\"player\")\n",
    "\n",
    "        for p in range(len(players)):\n",
    "            div = players[p].find(\"div\", class_= \"name\")\n",
    "            # https://stackoverflow.com/questions/55442727/remove-unicode-xa0-from-pandas-column\n",
    "            div_number = players[p].find(\"div\", class_= \"number\")\n",
    "            if div_number is not None:\n",
    "                div_number = div_number.get_text().strip()\n",
    "                if div_number is not '':\n",
    "                    player_number.append(div_number)\n",
    "\n",
    "            for a in div.find_all('a'):\n",
    "                # url to scrape\n",
    "                #player_number = p\n",
    "                player_url = a.get('href') #for getting link\n",
    "                player_id.append(player_url.split('=', 1)[1])\n",
    "                # player name\n",
    "                player_name.append(a.text) #for getting text between the link\n",
    "        return [match_id] * len(player_id), player_id, player_number, player_name\n",
    "    else:\n",
    "        # NOT SMART, this will fuck up the data types (PyTables pickle performance warning)\n",
    "        return [match_id], [-1], [None], [None]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile the workhorse function line by line. Virtually all of the time is now spend in BeautifulSoup, with 70% in the BS parser, and 30% in the find calls that filter out the stuff we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f extract_player_ids_from_single_file extract_player_ids_from_single_file(4386470)\n",
    "\n",
    "# 31% of the time is parsing the HTML!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_player_ids_from_single_file(4353200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the match id - player id list from the match HTML files\n",
    "\n",
    "This gives us for each match the participating player ids.\n",
    "Each match contains on average 25 players.\n",
    "So for 170K matches, we are looking at some 5M player records to create. \n",
    "This likely drops again as there will be lots of duplicates, as players are reused.\n",
    "\n",
    "Processing the HTML for 100 matches takes 24 s, \n",
    "Processing the HTML for 1000 matches takes 5 min, \n",
    "for 150K we are at 750 min. So roughly 12 hours.\n",
    "13K 725 min ???\n",
    "\n",
    "After profiling, now 1000 matches takes 60s and scales (down from 5 min, and increasing due to memory problems).\n",
    "The trick is to work with lists, and only in the end convert them to to a pandas dataframe.\n",
    "\n",
    "Expect up to 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_player_ids(N):\n",
    "\n",
    "    target = 'raw/df_player_ids_html_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5' # \n",
    "\n",
    "    print(target)\n",
    "\n",
    "    end_match = 4386470 #use from previous cell # \n",
    "    begin_match = 4216257 #\n",
    "\n",
    "    n_matches = end_match - begin_match\n",
    "    print(n_matches)\n",
    "    full_run = 1\n",
    "\n",
    "    #print(n_matches)\n",
    "\n",
    "    if(full_run):\n",
    "        match_ids = []\n",
    "        player_ids = []\n",
    "        player_numbers = []\n",
    "        player_names = []\n",
    "        \n",
    "        for i in range(n_matches):\n",
    "            match_id = end_match - i\n",
    "\n",
    "            # it spends 99% of the time in this function\n",
    "            match_id_tmp, player_id_tmp, player_number_tmp, player_name_tmp = extract_player_ids_from_single_file(match_id)\n",
    "\n",
    "            match_ids.extend(match_id_tmp) # use extend instead of + for efficiency\n",
    "            player_ids.extend(player_id_tmp)\n",
    "            player_numbers.extend(player_number_tmp)\n",
    "            player_names.extend(player_name_tmp)\n",
    "\n",
    "            if i % 1000 == 0: \n",
    "            # write progress report\n",
    "                print(i, end='')\n",
    "                print(\".\", end='')\n",
    "\n",
    "            if i % 1000 == 0:  \n",
    "            # write data as hdf5 file     \n",
    "                data = zip(match_ids, player_ids, player_numbers, player_names)\n",
    "\n",
    "                df_player_ids_html = pd.DataFrame(data, columns = ['match_id', 'player_id', 'player_number', 'player_name'])\n",
    "\n",
    "                df_player_ids_html.to_hdf(target, key='df_player_ids_html', mode='w')\n",
    "\n",
    "        # write data as hdf5 file\n",
    "        data = zip(match_ids, player_ids, player_numbers, player_names)\n",
    "\n",
    "        df_player_ids_html = pd.DataFrame(data, columns = ['match_id', 'player_id', 'player_number', 'player_name'])\n",
    "        df_player_ids_html.to_hdf(target, key='df_player_ids_html', mode='w')\n",
    "    else:\n",
    "        print(\"do nothing\")\n",
    "        # read from hdf5 file\n",
    "\n",
    "\n",
    "#extract_player_ids(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "#%lprun -f extract_player_ids extract_player_ids(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html = pd.read_hdf('raw/df_player_ids_html_20221229_121951.h5')\n",
    "df_player_ids_html.query('match_id == 4353201') # 4353065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_player_ids_html[df_player_ids_html.duplicated(['player_id'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html.match_id.nunique()\n",
    "\n",
    "df_player_ids_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html = df_player_ids_html.drop_duplicates(subset='player_id', keep='first')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the match HTML files\n",
    "\n",
    "I highly recommend [this tutorial](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/) for a great introduction to `BeautifulSoup`.\n",
    "It allows easy access to the information contained within HTML \"div\" tags that partition the web page in different sections. \n",
    "\n",
    "In addition, to clean up the scraped text, I used the **re** Python module (Regular expressions), part of the [Python standard library](https://docs.python.org/3/library/index.html) to extract the actual inducements from the text string that contains them.\n",
    "\n",
    "Processing 1000 matches takes 2 min, so to process all 150K matches is expected to take 300 min (in the end it took 800+ min).\n",
    "\n",
    "Now it takes 1.5 min. 40K matches, expect 1h. check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_the_stuff():\n",
    "\n",
    "    target = 'raw/df_matches_html_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "\n",
    "    print(target)\n",
    "\n",
    "    end_match = 4429319 \n",
    "    begin_match = 4386470\n",
    "\n",
    "    n_matches = end_match - begin_match\n",
    "    full_run = 0\n",
    "\n",
    "    print(n_matches)\n",
    "\n",
    "    if(full_run):\n",
    "        match_id = [] # used CTRL + D to add to selection\n",
    "        team1_inducements = []\n",
    "        team2_inducements = []\n",
    "        coach1_ranking = []\n",
    "        coach2_ranking = []\n",
    "        team1_comp = []\n",
    "        team2_comp = []\n",
    "        team1_pass = []\n",
    "        team2_pass = []\n",
    "        team1_rush = []\n",
    "        team2_rush = []\n",
    "        team1_block = []\n",
    "        team2_block = []\n",
    "        team1_foul = []\n",
    "        team2_foul = []\n",
    "\n",
    "        for i in range(n_matches):\n",
    "            match_id_tmp = end_match - i\n",
    "            dirname = \"raw/match_html_files/\" + str(match_id_tmp)[0:4]\n",
    "\n",
    "            # PM first check gz, if not exist then check for unzipped html\n",
    "            fname_string = dirname + \"/match_\" + str(match_id_tmp) + \".html\"        \n",
    "            fname_string_gz = dirname + \"/match_\" + str(match_id_tmp) + \".html.gz\"        \n",
    "\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                soup = BeautifulSoup(f, 'xml')\n",
    "\n",
    "            if soup.find(\"div\", {\"class\": \"matchrecord\"}) is not None:\n",
    "                # match record is available\n",
    "                inducements = soup.find_all(\"div\", class_=\"inducements\")\n",
    "\n",
    "                pattern = re.compile(r'\\s+Inducements: (.*)\\n')\n",
    "\n",
    "                match = re.match(pattern, inducements[0].get_text())\n",
    "                if match:\n",
    "                    team1_inducements_tmp = match.group(1)\n",
    "                else:\n",
    "                    team1_inducements_tmp = ''\n",
    "\n",
    "                match = re.match(pattern, inducements[1].get_text())\n",
    "                if match:\n",
    "                    team2_inducements_tmp = match.group(1)\n",
    "                else:\n",
    "                    team2_inducements_tmp = ''\n",
    "\n",
    "                coach_info = soup.find_all(\"div\", class_=\"coach\")\n",
    "                # grab the ranking\n",
    "                coach1_ranking_tmp = coach_info[0].get_text()\n",
    "                coach2_ranking_tmp = coach_info[1].get_text()\n",
    "\n",
    "                # match performance stats\n",
    "                div = soup.find_all('div', class_= \"player foot\")\n",
    "                # passing completions\n",
    "                regex = re.compile('.*front comp statbox.*')\n",
    "                team1_comp_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_comp_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()                   \n",
    "                # passing distance in yards\n",
    "                regex = re.compile('.*back pass statbox.*')\n",
    "                team1_pass_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_pass_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # rushes\n",
    "                regex = re.compile('.*back rush statbox.*')\n",
    "                team1_rush_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_rush_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # block\n",
    "                regex = re.compile('.*back block statbox.*')\n",
    "                team1_block_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_block_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # foul\n",
    "                regex = re.compile('.*back foul statbox.*')\n",
    "                team1_foul_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_foul_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "\n",
    "                match_id.append(match_id_tmp) # append for single item, extend for multiple items\n",
    "                team1_inducements.append(team1_inducements_tmp)\n",
    "                team2_inducements.append(team2_inducements_tmp)\n",
    "                coach1_ranking.append(coach1_ranking_tmp)\n",
    "                coach2_ranking.append(coach2_ranking_tmp)\n",
    "                team1_comp.append(team1_comp_tmp)\n",
    "                team2_comp.append(team2_comp_tmp)\n",
    "                team1_pass.append(team1_pass_tmp)\n",
    "                team2_pass.append(team2_pass_tmp)\n",
    "                team1_rush.append(team1_rush_tmp)\n",
    "                team2_rush.append(team2_rush_tmp)\n",
    "                team1_block.append(team1_block_tmp)\n",
    "                team2_block.append(team2_block_tmp)\n",
    "                team1_foul.append(team1_foul_tmp)\n",
    "                team2_foul.append(team2_foul_tmp)\n",
    "            \n",
    "            if i % 1000 == 0: \n",
    "            # write progress report\n",
    "                print(i, end='')\n",
    "                print(\".\", end='')\n",
    "\n",
    "                data = zip(match_id, team1_inducements, team2_inducements, \n",
    "                                        coach1_ranking, coach2_ranking, team1_comp, team2_comp,\n",
    "                                        team1_pass, team2_pass, team1_rush, team2_rush,\n",
    "                                        team1_block, team2_block, team1_foul, team2_foul)\n",
    "\n",
    "                df_matches_html = pd.DataFrame(data, columns = ['match_id', 'team1_inducements', 'team2_inducements',\n",
    "                'coach1_ranking', 'coach2_ranking', 'team1_comp', 'team2_comp',\n",
    "                'team1_pass', 'team2_pass', 'team1_rush', 'team2_rush',\n",
    "                'team1_block', 'team2_block', 'team1_foul', 'team2_foul'])\n",
    "\n",
    "                df_matches_html.to_hdf(target, key='df_matches_html', mode='w')\n",
    "\n",
    "        # write data as hdf5 file\n",
    "        data = zip(match_id, team1_inducements, team2_inducements, \n",
    "                                coach1_ranking, coach2_ranking, team1_comp, team2_comp,\n",
    "                                team1_pass, team2_pass, team1_rush, team2_rush,\n",
    "                                team1_block, team2_block, team1_foul, team2_foul)\n",
    "\n",
    "        df_matches_html = pd.DataFrame(data, columns = ['match_id', 'team1_inducements', 'team2_inducements',\n",
    "        'coach1_ranking', 'coach2_ranking', 'team1_comp', 'team2_comp',\n",
    "        'team1_pass', 'team2_pass', 'team1_rush', 'team2_rush',\n",
    "        'team1_block', 'team2_block', 'team1_foul', 'team2_foul'])\n",
    "\n",
    "        df_matches_html.to_hdf(target, key='df_matches_html', mode='w')\n",
    "    else:\n",
    "        # read from hdf5 file    \n",
    "        df_matches_html1 = pd.read_hdf('raw/df_matches_html_20220315_220825.h5') \n",
    "        df_matches_html2 = pd.read_hdf('raw/df_matches_html_20220316_133752.h5') \n",
    "        df_matches_html3 = pd.read_hdf('raw/df_matches_html_20220608_054453.h5')\n",
    "        df_matches_html4 = pd.read_hdf('raw/df_matches_html_20230115_133734.h5')\n",
    "        df_matches_html = pd.concat([df_matches_html1, df_matches_html2, df_matches_html3, df_matches_html4], ignore_index= True)\n",
    "\n",
    "    return df_matches_html\n",
    "\n",
    "df_matches_html = do_all_the_stuff()\n",
    "\n",
    "df_matches_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%lprun -f do_all_the_stuff do_all_the_stuff()\n",
    "# df_matches_html_20230115_133734\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of the profiling: >95% of the time is spend within Beautiful Soup. Switch to xml parser doubled processing speed.\n",
    "No further room for improvements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the performance stats\n",
    "\n",
    "The completions have a few weird values like `4/1`, we drop the slash and the value behind that.\n",
    "`-` is converted to 0 when the string ends directly after the `-` character, i.e. `-` becomes 0 but `-1` becomes -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['team1_comp'] = df_matches_html['team1_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team1_comp'] = pd.to_numeric(df_matches_html['team1_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team2_comp'] = df_matches_html['team2_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team2_comp'] = pd.to_numeric(df_matches_html['team2_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_pass'] = pd.to_numeric(df_matches_html['team1_pass'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_pass'] = pd.to_numeric(df_matches_html['team2_pass'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_rush'] = pd.to_numeric(df_matches_html['team1_rush'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_rush'] = pd.to_numeric(df_matches_html['team2_rush'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_block'] = pd.to_numeric(df_matches_html['team1_block'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_block'] = pd.to_numeric(df_matches_html['team2_block'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_foul'] = pd.to_numeric(df_matches_html['team1_foul'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_foul'] = pd.to_numeric(df_matches_html['team2_foul'].str.replace(r'-$','0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: coach rankings\n",
    "\n",
    "We want to extract the part `CR 149.99` from the scraped coach information field (example `test_string` below). Just as we matches on `Inducements:`, we can match on `CR ` and grab the contents directly after that, stopping when we encounter a whitespace.\n",
    "\n",
    "We first play around a bit and test until we discover the proper Regular Expression to use :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'kingcann Emerging Star CR 149.99 (-0.76)'\n",
    "\n",
    "pattern = re.compile(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "match = re.match(pattern, test_string)\n",
    "\n",
    "if match is not None:\n",
    "    print(match.group(1)) # group(0) is the whole string\n",
    "else:\n",
    "    print(\"match is none\")\n",
    "\n",
    "test_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 'm! Now that we have figured it out, we can write the code that extracts the coach rankings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataprep fix match_id\n",
    "df_matches_html['match_id'] = pd.to_numeric(df_matches_html.match_id) \n",
    "\n",
    "# Dataprep: add the coach rankings as separate cols\n",
    "df_matches_html['coach1_CR'] = df_matches_html['coach1_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "df_matches_html['coach2_CR'] = df_matches_html['coach2_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "df_matches_html['coach1_CR'] = pd.to_numeric(df_matches_html['coach1_CR'])\n",
    "df_matches_html['coach2_CR'] = pd.to_numeric(df_matches_html['coach2_CR'])\n",
    "\n",
    "# abs\n",
    "df_matches_html['CR_diff'] = np.abs(df_matches_html['coach1_CR'] - df_matches_html['coach2_CR'])\n",
    "df_matches_html['CR_diff'] = df_matches_html['CR_diff'].astype(float)\n",
    "\n",
    "# +/-\n",
    "df_matches_html['cr_diff2'] = df_matches_html['coach1_CR'] - df_matches_html['coach2_CR']\n",
    "\n",
    "df_matches_html['cr_diff2_bin'] = pd.cut(df_matches_html['cr_diff2'], bins = [-1*float(\"inf\"), -30, -20, -10, -5, 5, 10, 20, 30, float(\"inf\")], \n",
    " labels=['{-Inf,-30]', '[-30,-20]', '[-20,-10]', '[-10,-5]', '[-5,5]', '[5,10]', '[10,20]', '[20,30]', '[30,Inf]']) \n",
    "\n",
    "df_matches_html['coach1_CR_bin'] = pd.cut(df_matches_html['coach1_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n",
    "\n",
    "df_matches_html['coach2_CR_bin'] = pd.cut(df_matches_html['coach2_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['coach1_CR_bin'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep match inducements for each team\n",
    "\n",
    "The next trick is to use `pandas` `explode()` method (similar to `separate_rows()` in `tidyverse` R) to give each inducement its own row in the dataset.\n",
    "This creates a dataframe (`inducements`) similar to `df_mbt` with each match generating at least two rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1_inducements = df_matches_html[['match_id', 'team1_inducements']]\n",
    "team2_inducements = df_matches_html[['match_id', 'team2_inducements']]\n",
    "\n",
    "# make column names equal\n",
    "team1_inducements.columns = team2_inducements.columns = ['match_id', 'inducements']\n",
    "team1_inducements['team'] = 'team1'\n",
    "team2_inducements['team'] = 'team2'\n",
    "\n",
    "# row bind the two dataframes\n",
    "inducements = pd.concat([team1_inducements, team2_inducements], ignore_index = True)\n",
    "\n",
    "# convert comma separated string to list\n",
    "inducements['inducements'] = inducements['inducements'].str.split(',')\n",
    "\n",
    "# make each element of the list a separate row\n",
    "inducements = inducements.explode('inducements')\n",
    "\n",
    "# strip leading and trailing whitespaces\n",
    "inducements['inducements'] = inducements['inducements'].str.strip()\n",
    "\n",
    "# create \"star player\" label\n",
    "inducements['star_player'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Star player\"), 'star_player'] = 1\n",
    "\n",
    "# create \"card\" label\n",
    "inducements['special_card'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Card\"), 'special_card'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add match HTML data to df_matches\n",
    "\n",
    "Here we add `df_matches_html` to `df_matches`. This contains each players inducements as a single string, not convenient for analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_matches_html, on='match_id', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add specific inducement info to df_matches\n",
    "\n",
    "The `inducements` dataframe cannot easily be added to `df_matches`. We can however, extract information from `inducements` at the match level and add this to `df_matches`. Here, I show how to add a 1/0 flag `has_sp` that codes for if a match included any star player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = (inducements\n",
    "            .groupby(\"match_id\")\n",
    "            .agg(has_sp = (\"star_player\", \"max\"))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "df_matches = pd.merge(df_matches, df_sp, on = \"match_id\", how = \"left\")\n",
    "\n",
    "df_matches['match_id'] = pd.to_numeric(df_matches.match_id) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: API scraping of team data\n",
    "\n",
    "Great! Almost there. There is still something missing though.\n",
    "\n",
    "FUMBBL allows coaches to create their own rulesets to play their own leagues and tournaments with. For example, there is a so-called \"Secret League\" where coaches can play with \"Ninja halflings\", \"Ethereal\" spirits etc. Instead of plain normal regular \"Halflings\" and \"Shambling Undead\" :-)\n",
    "\n",
    "Since we want the team strength for the official rulesets BB2016 and BB2020, we need to distinguish those matches from the matches that are played under different rules.\n",
    "We need to know, for all the matches in our `df_matches` dataset, in what `division` or `league` the match took place, and what version of the rules (encoded in `ruleset`) was used. This information is available in the FUMBBL API, but not on the match level but on the **team** level.\n",
    "\n",
    "So, let us grab for all teams in `df_matches` the team `division`, `division_id`, `league`, and `ruleset` (last two are both numbers).\n",
    "\n",
    "As most other available information through the team API can vary over time (i.e. number of games played, how may rerolls a team has, or whether it has an apothecary), we do no fetch this information (expect number of games played), as we cannot link it to specific matches. We only fetch the information we expect to be valid for **all matches** played by this team. \n",
    "\n",
    "A limitation of the FUMBBL API is that it shows only the latest version of the teams and leagues data.  \n",
    "\n",
    "This hides the fact that leagues have changed their rules since they were first created. For example, the NAF used BB2016 rules up until summer of 2021, and thereafter switched to the new BB2020 ruleset for their latest online tournament.\n",
    "So we have to use our \"domain knowledge\" here to interpret the data properly.\n",
    "\n",
    "**PM we now have tournament id  as well, possibly this allows to at least pinpoint when rulesets might have changed**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just get all the teams for local storage.\n",
    "PM Next time, write code that checks for presence of local team files, and if not present query API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of all teams that need to be fetched\n",
    "#team_ids = list(df_matches['team1_id'].dropna()) + list(df_matches['team2_id'].dropna())\n",
    "\n",
    "all_team_ids = (df_matches\n",
    ".loc[:,['team1_id']]\n",
    ".team1_id.tolist()\n",
    ") +  (df_matches\n",
    ".loc[:,['team2_id']]\n",
    ".team2_id.tolist()\n",
    ")\n",
    "\n",
    "# get unique values by converting to a Python set and back to list\n",
    "all_team_ids = list(set(all_team_ids))\n",
    "\n",
    "len(all_team_ids)\n",
    "\n",
    "team_ids_to_fetch = all_team_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first we only get the team data and store it on disk as gzipped JSON\n",
    "\n",
    "each team takes 1 s, so we expect for 20K teams\n",
    "\n",
    "4K per hour, 5 hours.\n",
    "500 min 30K teams. \n",
    "\n",
    "75K teams, took 1.5 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save team API call as gzipped JSON object.\n",
    "# Combine with https://fumbbl.com/api/team/getOptions/1038960\n",
    "\n",
    "print(\"teams to scrape: \")\n",
    "n_teams = len(team_ids_to_fetch)\n",
    "\n",
    "print(n_teams)\n",
    "\n",
    "fullrun = 0\n",
    "\n",
    "if fullrun:\n",
    "    print('fetching team data for ', len(team_ids_to_fetch), ' teams')\n",
    "\n",
    "    for t in range(n_teams):\n",
    "        team_id = int(team_ids_to_fetch[t])\n",
    "            \n",
    "        api_string = \"https://fumbbl.com/api/team/get/\" + str(team_id)\n",
    "\n",
    "        response = requests.get(api_string)\n",
    "        response = response.json()\n",
    "\n",
    "        dirname = \"raw/team_files/\" + str(team_id)[0:4]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        fname_string = dirname + \"/team_\" + str(team_id) + \".json.gz\"\n",
    "        \n",
    "        with gzip.open(fname_string, mode = \"w\") as f:\n",
    "            f.write(json.dumps(response).encode('utf-8'))  \n",
    "            f.close()\n",
    "\n",
    "        time.sleep(0.3)\n",
    "        # also get tournament skills via getoptions\n",
    "        api_string = \"https://fumbbl.com/api/team/getOptions/\" + str(team_id)\n",
    "        response = requests.get(api_string)\n",
    "        response = response.json()\n",
    "        response['tournamentSkills'] = json.loads(response['tournamentSkills'])\n",
    "\n",
    "        fname_string = dirname + \"/team_\" + str(team_id) + \"_skills.json.gz\"\n",
    "        \n",
    "        with gzip.open(fname_string, mode = \"wb\") as f:\n",
    "            f.write(json.dumps(response).encode(\"utf-8\"))\n",
    "            f.close()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        if t % 1000 == 0: \n",
    "            # write progress report\n",
    "            print(t, end='')\n",
    "            print(\".\", end='')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process team data from disk\n",
    "\n",
    "So we have to process data for 5K different teams. \n",
    "\n",
    "We use the same approach as above, now looping over all `team_id` 's\n",
    "\n",
    "PM add all relevant team info that add to TV in tournament (rerolls , teamvalue , fan factor etc)\n",
    "\n",
    "this takes 1 min for 75K teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_the_team_stuff():\n",
    "    \n",
    "    n_teams = len(team_ids_to_fetch)\n",
    "    #n_teams = 100\n",
    "\n",
    "    target = 'raw/df_teams_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "    fullrun = 0\n",
    "\n",
    "    if fullrun:\n",
    "        team_id = []\n",
    "        division_id = []\n",
    "        division_name = []\n",
    "        league = []\n",
    "        ruleset = []\n",
    "        roster_id = []\n",
    "        race_name = []\n",
    "        games_played = []\n",
    "\n",
    "        print('processing team data for ', len(team_ids_to_fetch), ' teams')\n",
    "\n",
    "        for t in range(n_teams):    \n",
    "            team_id_tmp = int(team_ids_to_fetch[t])\n",
    "            dirname = \"raw/team_files/\" + str(team_id_tmp)[0:4]\n",
    "\n",
    "            fname_string_gz = dirname + \"/team_\" + str(team_id_tmp) + \".json.gz\"        \n",
    "            \n",
    "            # PM read compressed json file\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                team = json.load(f)\n",
    "\n",
    "            # grab fields\n",
    "            team_id_tmp = team['id'] # = team_id_tmp\n",
    "            division_id_tmp = team['divisionId']\n",
    "            division_name_tmp = team['division']\n",
    "            ruleset_tmp = team['ruleset']\n",
    "            league_tmp = team['league']\n",
    "            roster_id_tmp = team['roster']['id']\n",
    "            race_name_tmp = team['roster']['name']\n",
    "            games_played_tmp = team['record']['games']\n",
    "\n",
    "            team_id.append(team_id_tmp)\n",
    "            division_id.append(division_id_tmp)\n",
    "            division_name.append(division_name_tmp)\n",
    "            league.append(league_tmp)\n",
    "            ruleset.append(ruleset_tmp)\n",
    "            roster_id.append(roster_id_tmp)\n",
    "            race_name.append(race_name_tmp)\n",
    "            games_played.append(games_played_tmp)       \n",
    "            \n",
    "            if t % 5000 == 0: \n",
    "                # write tmp data as hdf5 file\n",
    "                print(t, end='')\n",
    "                print(\".\", end='')\n",
    "                data = zip(team_id, division_id, division_name, league, ruleset, roster_id, race_name,  games_played)\n",
    "\n",
    "                df_teams = pd.DataFrame(data, columns=['team_id', 'division_id', 'division_name',  'league' ,\n",
    "        'ruleset', 'roster_id', 'race_name',  'games_played'])\n",
    "                df_teams.to_hdf(target, key='df_teams', mode='w')\n",
    "        \n",
    "        data = zip(team_id, division_id, division_name, league, ruleset, roster_id, race_name,  games_played)\n",
    "                \n",
    "        df_teams = pd.DataFrame(data, columns=['team_id', 'division_id', 'division_name',  'league' ,\n",
    "        'ruleset', 'roster_id', 'race_name',  'games_played'])\n",
    "        df_teams.to_hdf(target, key='df_teams', mode='w')\n",
    "\n",
    "    else:\n",
    "        # read from hdf5 file\n",
    "        #df_teams1 = pd.read_hdf('raw/df_teams_20220316_221902.h5')\n",
    "        #df_teams2 = pd.read_hdf('raw/df_teams_20220609_062756.h5')\n",
    "        df_teams = pd.read_hdf('raw/df_teams_20230122_091040.h5')\n",
    "        #df_teams = pd.concat([df_teams1, df_teams2], ignore_index=True)\n",
    "    return df_teams\n",
    "\n",
    "df_teams = do_all_the_team_stuff()\n",
    "\n",
    "df_teams['roster_name'] = df_teams['roster_id'].astype(str) + '_' + df_teams['race_name']\n",
    "\n",
    "df_teams.shape\n",
    "\n",
    "df_teams.info()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: Add ruleset_version and division_name to df_teams\n",
    "\n",
    "Lets have look at the various divisions and leagues, and which rulesets are used.\n",
    "There are a lot of small leagues being played on FUMBBL, they account for maybe X% of all the matches.\n",
    "\n",
    "We only look at divisions and leagues with a sufficient volume of matches, or otherwise we do not have sufficient statistics for each race.\n",
    "\n",
    "So I aggregated the data by division, league and ruleset, and filtered on at least 150 different teams that have played at least once last year.\n",
    "Apart from the main \"Divisions\" that are part of FUMBBL, there were a few user-run leagues present in this table, so I looked up their names on FUMBBL and what ruleset is used (BB2016, BB2020 or some other variant). This information (contained in an xlsx) is added to the dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ruleset_version and division_name from codelist CSV\n",
    "ruleset_division_names = pd.read_csv('codelists/ruleset_division_names.csv')\n",
    "\n",
    "# initial creation of the codelist\n",
    "#ruleset_division_names.to_csv('codelists/ruleset_division_names.csv', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "df_teams = pd.merge(df_teams, ruleset_division_names, on= ['league', 'ruleset', 'division_id'], how='left')\n",
    "\n",
    "df_teams['division_name'] = df_teams['new_division_name']\n",
    "\n",
    "df_teams = df_teams.drop('new_division_name', 1)\n",
    "\n",
    "df_teams['division_id'] = pd.to_numeric(df_teams.division_id) \n",
    "df_teams['roster_id'] = pd.to_numeric(df_teams.roster_id) \n",
    "df_teams['team_id'] = pd.to_numeric(df_teams.team_id) \n",
    "df_teams['games_played'] = pd.to_numeric(df_teams.games_played) \n",
    "\n",
    "df_teams['league'] = pd.to_numeric(df_teams.league) \n",
    "df_teams['ruleset'] = pd.to_numeric(df_teams.ruleset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset_division_names.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams = df_teams.drop(['group_id', 'details', 'tv','type'], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have labeled all high volume leagues, divisions and rulesets:\n",
    "\n",
    "**PM we see that (NAF) matches played previously under ruleset 2228 are now labeled as ruleset 2310?\n",
    "this has a few changes (tier, gold, crossleague)\n",
    "Do we also see this in the XML API**\n",
    "\n",
    "So a group / league has a current ruleset attached.\n",
    "This current ruleset is also presented / attached to the team.\n",
    "Is it also attached to a match? can we know under which ruleset a match played?\n",
    "\n",
    "matches are played within a tournament id. within a group.\n",
    "so there is a link between tournament and ruleset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_teams\n",
    "    .groupby(['ruleset', 'league', 'division_id', 'division_name',  'ruleset_version'], dropna=False)\n",
    "    .agg( n_teams = ('ruleset', 'count')\n",
    "    )\n",
    "    .sort_values('n_teams', ascending = False)\n",
    "    .query('n_teams > 150')['n_teams']\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: Merging the division/ruleset data with the match data\n",
    "\n",
    "As most of the data in `df_teams` is actually on the level of the match, we can merge on `team1_id` after leaving out the team specific variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_teams.drop(['race_name', 'roster_id', 'roster_name', 'games_played'], 1), left_on='team1_id', right_on = 'team_id', how='left')\n",
    "\n",
    "df_matches['team1_id'] = pd.to_numeric(df_matches.team1_id) \n",
    "df_matches = df_matches.drop('team_id', 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create matches by team DataFrame\n",
    "\n",
    "When analyzing the data, we also like to have a dataframe `df_mbt (df_matches_by_team)` that contains, for each match, a separate row for each team participating in that match.\n",
    "This structure is nicely visualized [at the Nufflytics blog](https://www.nufflytics.com/post/the-value-of-tv/).\n",
    "Such a dataset is suitable for adding, at the match level, data that is specific for each team - coach pair, such as team value, coach rating etc.\n",
    "For example, we can imagine adding more team level data, such as casualties caused during the match, or team composition at the start of the match etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two copies, one for each team in the match\n",
    "team1_data = df_matches[['match_id', 'match_date', 'week_number',\t'year',\t'week_year', 'week_date', 'team1_id', 'ruleset', 'league', 'division_id', 'division_name', 'ruleset_version',\n",
    "    'team1_coach_id', 'team1_race_name', 'team1_value', 'team1_score', 'team1_win', 'tv_diff', 'tv_match', 'team2_coach_id', 'team2_race_name', 'team2_value', 'team2_score', \n",
    "    'team1_comp', 'team1_pass', 'team1_rush', 'team1_block', 'team1_foul', 'team1_cas', 'team1_cas_bh', 'team1_cas_si', 'team1_cas_rip',\n",
    "    'team2_comp', 'team2_pass', 'team2_rush', 'team2_block', 'team2_foul', 'team2_cas', 'team2_cas_bh', 'team2_cas_si', 'team2_cas_rip',\n",
    "    'tv_bin', 'tv_bin2', 'mirror_match', 'coach1_CR', 'coach2_CR', 'coach1_CR_bin', 'CR_diff',  'has_sp']].copy()\n",
    "\n",
    "team2_data = df_matches[['match_id', 'match_date', 'week_number',\t'year',\t'week_year', 'week_date', 'team2_id', 'ruleset', 'league', 'division_id', 'division_name', 'ruleset_version',\n",
    "    'team2_coach_id', 'team2_race_name', 'team2_value', 'team2_score', 'team2_win', 'tv_diff', 'tv_match', 'team1_coach_id', 'team1_race_name', 'team1_value', 'team1_score',\n",
    "    'team2_comp', 'team2_pass', 'team2_rush', 'team2_block', 'team2_foul', 'team2_cas', 'team2_cas_bh', 'team2_cas_si', 'team2_cas_rip',\n",
    "    'team1_comp', 'team1_pass', 'team1_rush', 'team1_block', 'team1_foul', 'team1_cas', 'team1_cas_bh', 'team1_cas_si', 'team1_cas_rip',\n",
    "    'tv_bin', 'tv_bin2', 'mirror_match', 'coach2_CR', 'coach1_CR', 'coach2_CR_bin','CR_diff', 'has_sp']].copy()\n",
    "\n",
    "team1_data.columns = team2_data.columns = ['match_id', 'match_date', 'week_number',\t'year',\t'week_year', 'week_date', 'team_id', 'ruleset', 'league', 'division_id', 'division_name', 'ruleset_version',\n",
    "    'coach_id', 'race_name', 'team_value', 'team_score', 'wins', 'tv_diff', 'tv_match',  'away_coach_id', 'away_race_name', 'away_team_value', 'away_team_score',\n",
    "    'home_comp', 'home_pass', 'home_rush', 'home_block', 'home_foul',  'home_cas', 'home_cas_bh', 'home_cas_si', 'home_cas_rip',\n",
    "    'away_comp', 'away_pass', 'away_rush', 'away_block', 'away_foul',  'away_cas', 'away_cas_bh', 'away_cas_si', 'away_cas_rip',\n",
    "    'tv_bin', 'tv_bin2', 'mirror_match', 'coach_CR', 'away_coach_CR', 'coach_CR_bin','CR_diff', 'has_sp']\n",
    "\n",
    "# combine both dataframes\n",
    "df_mbt = pd.concat([team1_data, team2_data])\n",
    "\n",
    "df_mbt['tv_diff2'] = df_mbt['team_value'] - df_mbt['away_team_value']\n",
    "df_mbt['cr_diff2'] = df_mbt['coach_CR'] - df_mbt['away_coach_CR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add games played for each team\n",
    "\n",
    "df_mbt = pd.merge(df_mbt, df_teams[['team_id', 'games_played']], left_on='team_id', right_on = 'team_id', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding outcome weights\n",
    "\n",
    "One way to measure team strength is to calculate a win rate.\n",
    "If we want to calculate win rates, we need to decide how to weigh a draw.\n",
    "In Blood Bowl data analysis, it seems that a 2:1:0 (W / D / L) weighting scheme is most commonly used. \n",
    "So if we want to compare with others, it makes sense to adapt this scheme as well.\n",
    "If we divide these weights by two we get something that, if we average it, we can interpret as a win rate.\n",
    "\n",
    "This scheme has the advantage that the weighted average win percentage over all matches is always 50%, creating a nice reference point allowing conclusions such as \"this and that team has an x percent above average win percentage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbt.loc[df_mbt['wins'] == 0, 'wins'] = 0.5\n",
    "df_mbt.loc[df_mbt['wins'] == -1, 'wins'] = 0\n",
    "\n",
    "# convert to float\n",
    "df_mbt['wins'] = df_mbt['wins'].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, Lets have a look at our dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbt.query(\"coach_id == 255851\").sort_values('match_date')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: adding race classifications to df_mbt (team tiers, bash/dash/hybrid)\n",
    "\n",
    "There are team classifications for strength (tiers) and classifications that focus on play style (Bash/Dash/Hybrid).\n",
    "The Bash/Dash/Hybrid classification i use is based on this great [Nufflytics blog](https://www.nufflytics.com/post/bash-dash-hybrid-by-the-numbers/), and my own subjective choices.\n",
    "Here we add both to the `df_mbt` dataframe, as in this dataset each row is about a single team, instead of a pairing.\n",
    "\n",
    "According to [this article from the NAF from 2017](https://www.thenaf.net/2017/05/tiers/), already since 2010 efforts were made to balance things out a bit between the different team strengths. For example, the weaker teams get more gold to spend on players, or get more so-called \"Star player points\" to spend on skilling players up. \n",
    "\n",
    "According to [the NAF](https://www.thenaf.net/tournaments/information/tiers-and-tiering/), traditionally team tiering consists of three groups, with Tier 1 being the strongest teams, and tier 3 the weakest teams. \n",
    "\n",
    "The GW BB2020 rule book also contains three tier groups, that are similar to the NAF tiers: except for Humans and Old World Alliance. \n",
    "\n",
    "And in november 2021, Games Workshop published an update of the three tier groups, now with High Elves moving from tier 2 to tier 1, and Old World Alliance moving back to tier 2.\n",
    "\n",
    "Finally, I added an additional race classification into the well known bash/dash (agile)/hybrid / stunty classes.\n",
    "\n",
    "**PM Vampires is on other**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_tiers = pd.read_csv('codelists/race_tiers_mapping.csv')\n",
    "#race_tiers = race_tiers[ ['race_name', 'bb2020_tier', 'naf_tier', 'bb2020_nov21_tier', 'race_type']]\n",
    "#race_tiers = race_tiers.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bb2020 tiers\n",
    "df_mbt = pd.merge(df_mbt, race_tiers, on='race_name', how='left')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2b: API scraping the player data: df_players\n",
    "\n",
    "**PM NOT USED, WHY HERE**\n",
    "\n",
    "\n",
    "Now that we have for all matches the player_id's involved, we fetch the player data.\n",
    "Were mostly interested in the player position, so we can reconstruct the team roster for a specific match.\n",
    "We can also analyze the development of teams over time: which positionals are bought in what order?\n",
    "\n",
    "1K players takes 10 min, so 4K players 40 min.\n",
    "\n",
    "We use the API, as it contains identical info as the HTML page.\n",
    "\n",
    "https://fumbbl.com/api/player/get/13524599\n",
    "\n",
    "PM Need to add the match_id \n",
    "so we are facing 1M players to scrape.\n",
    "at a rate of 2 scrapes per second, this will take 6 days of permanent scraping.\n",
    "We'll subset on BB2020, this will take 3 days.\n",
    "170K matches takes 8gb of disk space.\n",
    "check filesize of a single player json object. Maybe compress?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json_file(json_object, player_id):\n",
    "\n",
    "    dirname = \"raw/player_html_files/\" + str(player_id)[0:4]\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    fname_string = dirname + \"/player_\" + str(player_id) + \".html\"\n",
    "\n",
    "    with open(fname_string, mode = \"w\", encoding='UTF-8') as f:\n",
    "        f.write(json.dumps(json_object, ensure_ascii=False, indent=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id = 14693304\n",
    "\n",
    "api_string = \"https://fumbbl.com/api/player/get/\" + str(player_id)\n",
    "\n",
    "player = requests.get(api_string)\n",
    "# PM here save JSON as file\n",
    "player = player.json()\n",
    "\n",
    "write_json_file(player, player_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_player_ids_html needed in bg\n",
    "def fetch_player_data_from_api(n_players):\n",
    "    full_run = 1\n",
    "    \n",
    "    if n_players > len(df_player_ids_html):\n",
    "        n_players = len(df_player_ids_html)\n",
    "    print(n_players)\n",
    "\n",
    "    for i in range(n_players):\n",
    "        player_id = df_player_ids_html.iloc[i].player_id\n",
    "        api_string = \"https://fumbbl.com/api/player/get/\" + str(player_id)\n",
    "        # PM add exception handling\n",
    "        player = requests.get(api_string)\n",
    "        # PM here save JSON as file\n",
    "        player = player.json()\n",
    "        write_json_file(player, player_id)\n",
    "        \n",
    "        if i % 100 == 0: \n",
    "            #show progress\n",
    "            print(i, end='')\n",
    "            print(\".\", end='')\n",
    "\n",
    "    return print(i+1, \"files written\")\n",
    "\n",
    "fetch_player_data_from_api(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the player JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(player_id):\n",
    "\n",
    "    dirname = \"raw/player_html_files/\" + str(player_id)[0:4]\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    fname_string = dirname + \"/player_\" + str(player_id) + \".html\"\n",
    "\n",
    "    with open(fname_string, mode = \"r\", encoding='UTF-8') as f:\n",
    "        json_object = json.load(f)\n",
    "\n",
    "    return json_object\n",
    "\n",
    "player = read_json_file(14693304)\n",
    "player['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to fetch the player position to check what skills are added.\n",
    "https://fumbbl.com/api/position/get/39328\n",
    "But this is the current set of skills, not necessary what skills were used in a particular game.\n",
    "\n",
    "For now we just go for position plots for say team values around 1200K.\n",
    "This will not work because the 1200K includes skills and we do not know them.\n",
    "\n",
    "Maybe check for tournaments with fixed skill choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_player_data_from_file(n_players):\n",
    "    target = 'raw/df_players_' + 'tst' + '.h5' # time.strftime(\"%Y%m%d_%H%M%S\") \n",
    "    print(target)\n",
    "\n",
    "    full_run = 1\n",
    "    \n",
    "    if n_players > len(df_player_ids_html):\n",
    "        n_players = len(df_player_ids_html)\n",
    "    print(n_players)\n",
    "\n",
    "    if(full_run):\n",
    "        player_ids = []\n",
    "        team_id = []\n",
    "        status = []\n",
    "        number = []\n",
    "        name = []\n",
    "        position_id = []\n",
    "        position_name = []\n",
    "        stats_ma = []\n",
    "        stats_st = []\n",
    "        stats_ag = []\n",
    "        stats_pa = []\n",
    "        stats_av = []\n",
    "\n",
    "        for i in range(n_players):\n",
    "            player_id = df_player_ids_html.iloc[i].player_id\n",
    "\n",
    "            player = read_json_file(player_id)\n",
    "            \n",
    "            if player: # fix for matches that do not exist\n",
    "                player_ids.append(player['id'])\n",
    "                team_id.append(player['teamId'])\n",
    "                status.append(player['status'])\n",
    "                number.append(player['number'])\n",
    "                name.append(player['name'])\n",
    "                position_id.append(player['position']['id'])\n",
    "                position_name.append(player['position']['name'])\n",
    "                stats_ma.append(player['stats']['ma'])\n",
    "                stats_st.append(player['stats']['st'])\n",
    "                stats_ag.append(player['stats']['ag'])\n",
    "                stats_pa.append(player['stats']['pa'])\n",
    "                stats_av.append(player['stats']['av'])\n",
    "                # PM skills, injuries: add in for loop (variable length)\n",
    "\n",
    "            else:\n",
    "                # empty data for this match, create empty row\n",
    "                print('An error has occurred.')\n",
    "                #df_players.loc[i] = np.repeat([np.NaN], 12, axis=0)\n",
    "                #df_players.loc[i]['player_id'] = int(player_id)\n",
    "            \n",
    "            if i % 100 == 0: \n",
    "                # write tmp data as hdf5 file\n",
    "                print(i, end='')\n",
    "                print(\".\", end='')\n",
    "                \n",
    "                data = zip(player_ids, team_id, status, number, name, \n",
    "                position_id, position_name,\n",
    "                stats_ma, stats_st, stats_ag, stats_pa,  stats_av)\n",
    "\n",
    "                df_players = pd.DataFrame(data, columns = ['player_id', 'team_id', 'status', 'number', 'name', \n",
    "                'position_id', 'position_name',\n",
    "                'stats_ma', 'stats_st', 'stats_ag', 'stats_pa',  'stats_av'\n",
    "                ])\n",
    "                df_players.to_hdf(target, key='df_players', mode='w')\n",
    "\n",
    "        # write data as hdf5 file\n",
    "        data = zip(player_ids, team_id, status, number, name, \n",
    "                position_id, position_name,\n",
    "                stats_ma, stats_st, stats_ag, stats_pa,  stats_av)\n",
    "\n",
    "        df_players = pd.DataFrame(data, columns = ['player_id', 'team_id', 'status', 'number', 'name', \n",
    "                'position_id', 'position_name',\n",
    "                'stats_ma', 'stats_st', 'stats_ag', 'stats_pa',  'stats_av'\n",
    "                ])\n",
    "        df_players.to_hdf(target, key='df_players', mode='w')\n",
    "    else:\n",
    "        # read from hdf5 file\n",
    "        print(\"do nothing\")\n",
    "    return df_players\n",
    "\n",
    "df_players = process_player_data_from_file(10)\n",
    "df_players = df_players.sort_values(by=['team_id']).reset_index(drop=True)\n",
    "df_players.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 add tournament info\n",
    "\n",
    "create list of all tournament ids.\n",
    "Fetch and store as JSON.\n",
    "Then process and add tournament names to `df_matches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of all teams that need to be fetched\n",
    "#team_ids = list(df_matches['team1_id'].dropna()) + list(df_matches['team2_id'].dropna())\n",
    "\n",
    "tournament_ids = df_matches['tournament_id'].values\n",
    "\n",
    "# get unique values by converting to a Python set and back to list\n",
    "tournament_ids = list(set(tournament_ids))\n",
    "\n",
    "len(tournament_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tournament API call as gzipped JSON object.\n",
    "\n",
    "print(\"tournaments to fetch: \")\n",
    "\n",
    "n_tourneys = len(tournament_ids)\n",
    "\n",
    "print(n_tourneys)\n",
    "\n",
    "fullrun = 0\n",
    "\n",
    "if fullrun:\n",
    "    print('fetching tournament data for ', n_tourneys, ' tourneys')\n",
    "\n",
    "    for t in range(n_tourneys):\n",
    "        tournament_id = int(tournament_ids[t])\n",
    "            \n",
    "        api_string = \"https://fumbbl.com/api/tournament/get/\" + str(tournament_id)\n",
    "\n",
    "        response = requests.get(api_string)\n",
    "        response = response.json()\n",
    "\n",
    "        dirname = \"raw/tournament_files/\" + str(tournament_id)[0:2]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        fname_string = dirname + \"/tournament_\" + str(tournament_id) + \".json.gz\"\n",
    "        \n",
    "        with gzip.open(fname_string, mode = \"w\") as f:\n",
    "            f.write(json.dumps(response).encode('utf-8'))  \n",
    "            f.close()\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        if t % 100 == 0: \n",
    "            # write progress report\n",
    "            print(t, end='')\n",
    "            print(\".\", end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process the tourney json files from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_the_tourney_stuff():\n",
    "    \n",
    "    n_tourneys = len(tournament_ids)\n",
    "    #n_tourneys = 100\n",
    "\n",
    "    target = 'raw/df_tourneys_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "    fullrun = 0\n",
    "\n",
    "    if fullrun:\n",
    "        tournament_id = []\n",
    "        group_id = []\n",
    "        tournament_type = []\n",
    "        tournament_progression = []\n",
    "        tournament_name = []\n",
    "        tournament_start = []\n",
    "        tournament_end = []\n",
    "\n",
    "        print('processing tourney data for ', n_tourneys, ' teams')\n",
    "\n",
    "        for t in range(n_tourneys):    \n",
    "            tournament_id_tmp = int(tournament_ids[t])\n",
    "            dirname = \"raw/tournament_files/\" + str(tournament_id_tmp)[0:2]\n",
    "\n",
    "            fname_string_gz = dirname + \"/tournament_\" + str(tournament_id_tmp) + \".json.gz\"        \n",
    "            \n",
    "            # PM read compressed json file\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                tournament = json.load(f)\n",
    "\n",
    "            if str(tournament) != 'No such tournament.':\n",
    "                # grab fields\n",
    "                tournament_id_tmp = tournament['id'] # = tournament_id_tmp\n",
    "                group_id_tmp = tournament['group']\n",
    "                tournament_type_tmp = tournament['type']\n",
    "                tournament_progression_tmp = tournament['progression']\n",
    "                tournament_name_tmp = tournament['name']\n",
    "                tournament_start_tmp = tournament['start']\n",
    "                tournament_end_tmp = tournament['end']\n",
    "\n",
    "                tournament_id.append(tournament_id_tmp)\n",
    "                group_id.append(group_id_tmp)\n",
    "                tournament_type.append(tournament_type_tmp)\n",
    "                tournament_progression.append(tournament_progression_tmp)\n",
    "                tournament_name.append(tournament_name_tmp)\n",
    "                tournament_start.append(tournament_start_tmp)\n",
    "                tournament_end.append(tournament_end_tmp)   \n",
    "\n",
    "            if t % 500 == 0: \n",
    "                # write tmp data as hdf5 file\n",
    "                print(t, end='')\n",
    "                print(\".\", end='')\n",
    "                data = zip(tournament_id, group_id, tournament_type, tournament_progression, tournament_name, tournament_start, tournament_end)\n",
    "\n",
    "                df_tourneys = pd.DataFrame(data, columns=['tournament_id', 'group_id', 'tournament_type', 'tournament_progression', \n",
    "                'tournament_name', 'tournament_start', 'tournament_end'])\n",
    "                df_tourneys.to_hdf(target, key='df_tourneys', mode='w')\n",
    "\n",
    "        data = zip(tournament_id, group_id, tournament_type, tournament_progression, tournament_name, tournament_start, tournament_end)\n",
    "        df_tourneys = pd.DataFrame(data, columns=['tournament_id', 'group_id', 'tournament_type', 'tournament_progression', \n",
    "        'tournament_name', 'tournament_start', 'tournament_end'])\n",
    "        df_tourneys.to_hdf(target, key='df_tourneys', mode='w')   \n",
    "\n",
    "\n",
    "    else:\n",
    "        # read from hdf5 file\n",
    "        df_tourneys = pd.read_hdf('raw/df_tourneys_20230122_135939.h5')\n",
    "\n",
    "    return df_tourneys\n",
    "\n",
    "df_tourneys = do_all_the_tourney_stuff()\n",
    "\n",
    "df_tourneys.shape\n",
    "\n",
    "df_tourneys['tournament_id'] = pd.to_numeric(df_tourneys.tournament_id) \n",
    "df_tourneys['group_id'] = pd.to_numeric(df_tourneys.group_id) \n",
    "df_tourneys['tournament_type'] = pd.to_numeric(df_tourneys.tournament_type) \n",
    "\n",
    "df_tourneys.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourneys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM next time also fetch ruleset JSON files\n",
    "\n",
    "## add tournament name to match data\n",
    "\n",
    "include tournament tyep\n",
    "https://fumbbl.com/help:Tournament+Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_tourneys[['tournament_id', 'tournament_name', 'tournament_type']], left_on='tournament_id', right_on = 'tournament_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.query(\"match_id == 4421729\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 grab Skill code list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run read_json_file.py\n",
    "%run write_json_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_string = \"https://fumbbl.com/api/skill/list\"\n",
    "\n",
    "skill_list = requests.get(api_string)\n",
    "skill_list = skill_list.json()\n",
    "\n",
    "write_json_file(skill_list, fname = \"raw/skill_list_bb2016.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_string = \"https://fumbbl.com/api/skill/list/2020\"\n",
    "\n",
    "skill_list = requests.get(api_string)\n",
    "skill_list = skill_list.json()\n",
    "\n",
    "write_json_file(skill_list, fname = \"raw/skill_list_bb2020.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert skill code JSONs into a single pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = read_json_file(fname = \"raw/skill_list_bb2020.json\")\n",
    "skill_list = pd.json_normalize(skill_list)\n",
    "skill_list.rename({'id' : 'skill_id'}, axis = 1, inplace=True)\n",
    "\n",
    "skill_list = skill_list.loc[:, ['skill_id', 'name']]\n",
    "\n",
    "skill_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list2 = read_json_file(fname = \"raw/skill_list_bb2016.json\")\n",
    "skill_list2 = pd.json_normalize(skill_list2)\n",
    "skill_list2.rename({'id' : 'skill_id'}, axis = 1, inplace=True)\n",
    "\n",
    "skill_list2 = skill_list2.loc[:, ['skill_id', 'name']]\n",
    "\n",
    "skill_list2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skills = pd.concat([skill_list, skill_list2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skills.sort_values('skill_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all prepped datasets as HDF5 and CSV files\n",
    "\n",
    "HD5 files can be read by both Python and [R](https://cran.r-project.org/web/packages/hdf5r/index.html) and preserve column data types.\n",
    "\n",
    "**Update** the HDF5 schema used by pandas is highly specific to pandas, it is not designed for external use. An R code snippet is available but does not work with table format. For now using the CSVs for R is advised\n",
    "\n",
    "CSV files are the lingua franca across all data analysis software.\n",
    "\n",
    "A dataset release consists of three datasets:\n",
    "* A list of matches, identified by match_id\n",
    "* A list of matches by team, identified by match_id and team_id\n",
    "* A list of inducements by match_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to locate variables that cannot be serialized by hdf5\n",
    "#df_matches.loc[:, :'week_date'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_matches'\n",
    "\n",
    "df_matches.to_hdf(target + '.h5', key='df_matches', mode='w', format = 't',  complevel = 9)\n",
    "df_matches.to_csv(target + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_mbt'\n",
    "\n",
    "df_mbt.to_hdf(target + '.h5', key='df_mbt', mode='w', format = 't',  complevel = 9)\n",
    "df_mbt.to_csv(target + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/inducements'\n",
    "inducements.to_hdf(target + '.h5', key='inducements', mode='w', format = 't',  complevel = 9)\n",
    "inducements.to_csv(target + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_tourneys'\n",
    "\n",
    "df_tourneys.to_hdf(target + '.h5', key='df_tourneys', mode='w', format = 't',  complevel = 9)\n",
    "df_tourneys.to_csv(target + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_skills'\n",
    "\n",
    "df_skills.to_hdf(target + '.h5', key='df_skills', mode='w', format = 't',  complevel = 9)\n",
    "df_skills.to_csv(target + '.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a license for the public dataset\n",
    "\n",
    "An important part of making data publicly available is being explicit about what is allowed if people want to use the dataset.\n",
    "However, before we do so, we have to check if **we** are actually allowed to publish the data. This is explained nicely [in a blogpost by Elizabeth Wickes](https://datacarpentry.org/blog/2016/06/data-licensing).\n",
    "\n",
    "Since our data comes from the **FUMBBL.com** website, we check the [**Privacy policy**](https://fumbbl.com/p/privacy) where all users, including myself have agreed on when signing up. It contains this part which is specific to the unauthenticated API, which we use to fetch the data, as well as additional public match data, such as which inducements are used in a match, and the Coach rankings of the playing coaches that were current when the match was played.\n",
    "\n",
    "```\n",
    "Content you provide through the website\n",
    "All the information you provide through the website is processed by FUMBBL. This includes things such as forum posts, private message posts, blog entries, team and player names and biographies and news comments. Data provided this way is visible by other people on the website and in most cases public even to individuals without accounts (not including private messages), and as such are considered of public interest. If direct personal information is posted in public view, you can contact moderators to resolve this. Match records are also considered content in this context, and is also considered of public interest. This data is collected as the primary purpose of the website and it is of course entirely up to you how much of this is provided to FUMBBL. \n",
    "\n",
    "Third party sharing\n",
    "Some of the public data is available through a public (*i.e. unauthenticated*) API, which shares some of the information provided by FUMBBL users in a way suitable for third-party websites and services to process.\n",
    "\n",
    "The data available through the unauthenticated API is considered non-personal as it only reflects information that is public by its nature on the website. The authenticated API will only show information connected to the authenticated account.\n",
    "```\n",
    "\n",
    "I conclude that since the match data is already considered public content, there is no harm in collecting this public data in a structured dataset and placing this data in a public repository. I also verified this with Christer, the site owner. \n",
    "\n",
    "\n",
    "The final step is then to decide what others are allowed to do with this data. In practice, this means choosing a license under which to release the dataset. I decided to choose a CC0 license: this places the data in the public domain, and people can use the dataset as they wish. Citing or mentioning the source of the data would still be appreciated of course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of possible future improvements\n",
    "\n",
    "* Scraping the players (only most recent version, so no player development history)\n",
    "* Scraping the rulesets (for example to identify resurrection tournaments where players choose skills and use tiers)\n",
    "* Switch to feather or Parquet dataformat\n",
    "* catch exception: **PM we cannot deal yet with the situation HTTPSConnectionPool(host='fumbbl.com', port=443): Max retries exceeded with url: /api/match/get/4221820 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4acff12be0>: Failed to establish a new connection: [Errno 110] Connection timed out',))**\n",
    "* PM we now have tournament id  as well, possibly this allows to at least pinpoint when rulesets might have changed\n",
    "* PM we see that (NAF) matches played previously under ruleset 2228 are now labeled as ruleset 2310? this has a few changes (tier, gold, crossleague)\n",
    "* Do we also see this in the XML API\n",
    "* cr_bin variable is gone?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50276fd1884268afe39607052f22ef19b84d915691d702a5c7e9a67a09867105"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('requests_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
